# physical-ai-papers

## 1. Action Models

### 1.1 Vision-Language-Action (VLA) Model

- **Gemini Robotics: Bringing AI into the Physical World** (Gemini Robotics Team, Google DeepMind, 2025)
[paper](https://arxiv.org/pdf/2503.20020), [blog](https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/)
- **GR00T N1: An Open Foundation Model for Generalist Humanoid Robots** (NVIDIA, 2025) [paper](https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf)
- **OpenVLA: An Open-Source Vision-Language-Action Model** (Stanford, 2024) [paper](https://arxiv.org/pdf/2406.09246), [github](https://github.com/openvla/openvla), [project](https://openvla.github.io/), [model](https://huggingface.co/openvla)
- **Octo: An Open-Source Generalist Robot Policy** (UC Berkeley, 2024) [paper](https://arxiv.org/pdf/2405.12213v2), [project](https://octo-models.github.io/)
- **TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation** (Wen et al., IEEE Robotics and Automation Letters 2024) [paper](https://arxiv.org/pdf/2409.12514), [project](https://tiny-vla.github.io/)
- **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control** (Google DeepMind, 2023) [paper](https://arxiv.org/pdf/2307.15818), [project](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)
- **$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control** (Physical Intelligence, 2024) [paper](https://www.physicalintelligence.company/download/pi0.pdf), [blog](https://www.physicalintelligence.company/blog/pi0)
- **SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data** (Huggingface, 2025) [paper](https://arxiv.org/pdf/2506.01844), [huggingface](https://huggingface.co/blog/smolvla)
- **Helix** (Figure AI, 2025) [post](https://www.figure.ai/news/helix), [post](https://www.figure.ai/news/scaling-helix-logistics)
- **A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation** (Toyota Research Institute, 2025) [project](https://toyotaresearchinstitute.github.io/lbm1/), [paper](https://arxiv.org/pdf/2507.05331)
- **(Survey) Foundation Models in Robotics: Applications, Challenges, and the Future** [paper](https://arxiv.org/pdf/2312.07843)
- **Robotic Control via Embodied Chain-of-Thought Reasoning** (UC Berkeley, 2025) [paper](https://arxiv.org/pdf/2407.08693), [project](https://embodied-cot.github.io/)
- **$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization** (Physical Intelligence, 2025) [paper](https://www.physicalintelligence.company/download/pi05.pdf), [blog](https://www.physicalintelligence.company/blog/pi05), [model](https://huggingface.co/Embodied-CoT)
- **Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models** (Physical Intelligence, 2025) [paper](https://www.physicalintelligence.company/blog/pi05), [blog](https://www.pi.website/research/hirobot)
- **FAST: Efficient Action Tokenization for Vision-Language-Action Models** (Physical Intelligence, 2025) [paper](https://arxiv.org/pdf/2501.09747), [blog](https://www.pi.website/research/fast)
- **Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks** (Alibaba Group, 2025) [paper](https://arxiv.org/pdf/2503.21696), [project](https://embodied-reasoner.github.io/)
- **Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous Robot Skills** (CMU and Tsinghua University, 2024) [paper](https://arxiv.org/pdf/2405.11380), [paper](https://meta-control-paper.github.io/)
- **Learning Interactive Real-World Simulators** (UC Berkeley, Google DeepMind, MIT and University of Alberta, ICRL 2024) [paper](https://arxiv.org/pdf/2310.06114), [project](https://universal-simulator.github.io/unisim/)
- **Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation** (AgiBot 2025) [paper](https://arxiv.org/pdf/2508.05635), [project](https://genie-envisioner.github.io/), [code](https://github.com/AgibotTech/Genie-Envisioner)
- **GR-3 Technical Report** (ByteDance 2025 [paper](https://arxiv.org/pdf/2507.15493), [project](https://seed.bytedance.com/en/GR3)
- **A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation** (Toyota Research Institute, 2025) [paper](https://arxiv.org/pdf/2507.05331), [project](https://toyotaresearchinstitute.github.io/lbm1/)
- **Unified Vision-Language-Action Model** (Wang et al., 2025) [paper](https://arxiv.org/pdf/2506.19850), [project](https://robertwyq.github.io/univla.github.io/), [code](https://github.com/baaivision/UniVLA), [model](https://huggingface.co/Yuqi1997/UniVLA)
- **RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models** (University of Pennsylvania, CoRL 2025) [paper](https://arxiv.org/pdf/2508.02062), [project](https://ricl-vla.github.io/), [github](https://github.com/ricl-vla/ricl_openpi)

### 1.2 Action Reasoning Model (ARM)

- **MolmoAct: An Action Reasoning Model that reasons in 3D space** (Allen AI, 2025), [model](https://huggingface.co/collections/allenai/molmoact-689697591a3936fba38174d7), [dataset](https://huggingface.co/collections/allenai/molmoact-data-mixture-6897e583e13b6c2cf3ea2b80), [blog](https://allenai.org/blog/molmoact)

### 1.3 Diffusion Policy (Diffusion-based Action Model)

- **Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead Control** (UC Berkeley, 2025) [paper](https://arxiv.org/pdf/2503.11801), [project](https://diffusecloc.github.io/website/)
- **(Survey) A Survey on Diffusion Policy for Robotic Manipulation: Taxonomy, Analysis, and Future Directions** (Song et al., 2025) [paper](https://d197for5662m48.cloudfront.net/documents/publicationstatus/252556/preprint_pdf/ff7315a28c7f1f75d985389927fcb645.pdf)
- **Diffusion Policy Policy Optimization** (Princeton University, CMU, 2024) [paper](https://arxiv.org/pdf/2409.00588), [project](https://diffusion-ppo.github.io/), [github](https://github.com/irom-princeton/dppo)
- **(Medical) SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants** (University of Toronto, IROS 2024) [paper](https://arxiv.org/pdf/2405.05226), [project](https://orbit-surgical.github.io/sufia/)
- **Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation** (Tsinghua University, 2025) [paper](https://arxiv.org/pdf/2503.02881)

## 2. Data Collection

### 2.1 Data Collection & Training

- **Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning** (Georgia Institute of Technology, 2025) [paper](https://arxiv.org/abs/2503.04877), [project](http://www.pair.toronto.edu/Adapt3R/), [github](https://github.com/pairlab/Adapt3R)
- **Robot Trains Robot: Automatic Real-World Policy: Adaptation and Learning for Humanoids** (Stanford, CoRL 2025) [paper](https://robot-trains-robot.github.io/rtr.pdf), [project](https://robot-trains-robot.github.io/), [github](https://github.com/hukz18/Robot-Trains-Robot)
- **BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion** (UC Berkeley, 2025) [paper](https://arxiv.org/pdf/2508.08241), [project](https://beyondmimic.github.io/)
- **Masquerade: Learning from In-the-wild Human Videos using Data-Editing** (Stanford, 2025) [paper](https://arxiv.org/pdf/2508.09976), [project](https://masquerade-robot.github.io/)
- [linkedIn](https://www.linkedin.com/posts/ilir-aliu_robotics-has-a-billion-dollar-failure-problem-activity-7360558587842039809-I7cA?utm_source=share&utm_medium=member_desktop&rcm=ACoAADiwsCgBQYl9HXlyHzfMMl7jL6de1VcuAqs)

## 3. Data Representation and World Model

### 3.1 World Model

## 4. Benchmark and Simulator

### 4.1 Simulator

- **(Medical) ORBIT-Surgical: An Open-Simulation Framework for Learning Surgical Augmented Dexterity** (University of Toronto, UC Berkeley, NVIDIA, ICRA 2024) [paper](https://arxiv.org/pdf/2404.16027), [project](https://orbit-surgical.github.io/), [code](https://github.com/orbit-surgical/orbit-surgical), [blog](https://blogs.nvidia.com/blog/orbit-surgical-robotics-research-icra/)

### 4.2 Benchmark

- **BEHAVIOR-1K** (Stanford) [project](https://behavior.stanford.edu/index.html#setup), [github](https://github.com/StanfordVL/BEHAVIOR-1K)

## 5. Backbone Models

### 5.1 Diffusion

- **Physics-Informed Diffusion Models** (ETH Zurich, 2025) [paper](https://arxiv.org/pdf/2403.14404)

## 6. ETC

- **Hearing the Slide: Acoustic-Guided Constraint Learning for Fast Non-Prehensile Transport** (CMU, 2025) [paper](https://fast-non-prehensile.github.io/content/hearing_the_slide.pdf), [project](https://fast-non-prehensile.github.io/)
